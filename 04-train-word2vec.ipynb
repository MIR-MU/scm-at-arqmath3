{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d232a003-836f-4940-86b7-3be5da304a8b",
   "metadata": {},
   "source": [
    "# Train `word2vec` models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d74700-e047-4d0b-a0bd-77597eb77e2b",
   "metadata": {},
   "source": [
    "In this notebook, we will fine-tune the `word2vec` model, so that it can represent math-specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a970c4d0-4f9e-4070-9bcf-29ceff2d1656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apollo.fi.muni.cz\n"
     ]
    }
   ],
   "source": [
    "! hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b123fd-1783-48a2-8816-ed9768fa1d1e",
   "metadata": {},
   "source": [
    "## Train non-positional `word2vec` models\n",
    "\n",
    "In this section, we will produce word embeddings for global `word2vec` models without [positional weighting][1].\n",
    "\n",
    " [1]: https://github.com/MIR-MU/pine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca548959-2b09-4408-a409-de40d4671c15",
   "metadata": {},
   "source": [
    "### The text + LaTeX format\n",
    "\n",
    "As our primary representation, we use text and LaTeX separated by special `[MATH]` and `[/MATH]` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b52b98-714f-4680-abf2-d9d6cf69a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make word2vec-text+latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846be871-a87a-4c12-b26e-53927c9c42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473M\tword2vec-text+latex/model/custom-en-word2vec_cbow-epochs=10\n",
      "473M\tword2vec-text+latex/model\n",
      "0\tword2vec-text+latex/cache/custom-en-word2vec_cbow-epochs=10\n",
      "4,0K\tword2vec-text+latex/cache\n",
      "473M\tword2vec-text+latex\n",
      "473M\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -hc word2vec-text+latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e8508-8646-48d1-b35b-924fd0768ee2",
   "metadata": {},
   "source": [
    "### The LaTeX format\n",
    "\n",
    "To train a `word2vec` model just for math, we also have a separate dataset with just LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7fb3a4c-1796-4c48-a896-6328fc452b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make word2vec-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92ea198-929e-4601-8a55-fbab4d39fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202M\tword2vec-latex/model/custom-en-word2vec_cbow-epochs=50\n",
      "202M\tword2vec-latex/model\n",
      "0\tword2vec-latex/cache/custom-en-word2vec_cbow-epochs=50\n",
      "4,0K\tword2vec-latex/cache\n",
      "202M\tword2vec-latex\n",
      "202M\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -hc word2vec-latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a1d58-877a-43d5-a34f-0f807b90b2a1",
   "metadata": {},
   "source": [
    "### The Tangent-L format\n",
    "\n",
    "To train a word2vec model just for math, we also have a separate dataset with just the format used by [the Tangent-L search engine from UWaterloo][1].\n",
    "\n",
    " [1]: http://ceur-ws.org/Vol-2936/paper-05.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1604ba54-cae9-4a9f-9247-1526c2eb284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make word2vec-tangentl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ac4a96-59fc-465c-ba0c-942ebb90f2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14G\tword2vec-tangentl/model/custom-en-word2vec_cbow-epochs=2\n",
      "14G\tword2vec-tangentl/model\n",
      "0\tword2vec-tangentl/cache/custom-en-word2vec_cbow-epochs=2\n",
      "4,0K\tword2vec-tangentl/cache\n",
      "14G\tword2vec-tangentl\n",
      "14G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -hc word2vec-tangentl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ae0e0-390f-4e7e-a4ee-1f3ed54cbe0a",
   "metadata": {},
   "source": [
    "## Train positional `word2vec` models\n",
    "\n",
    "In this section, we will produce word embeddings for global `word2vec` models with [positional weighting][1].\n",
    "\n",
    " [1]: https://github.com/MIR-MU/pine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa26a57-9351-4921-ba20-c52f28a2c503",
   "metadata": {},
   "source": [
    "### The text + LaTeX format\n",
    "\n",
    "As our primary representation, we use text and LaTeX separated by special `[MATH]` and `[/MATH]` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e3d10a-46f3-45ad-9def-d961162cb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make word2vec-text+latex-positional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c90bd9-f14e-474f-97b5-85f68cdebe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472M\tword2vec-text+latex-positional/model/custom-en-constrained_positional_word2vec_cbow-epochs=10\n",
      "472M\tword2vec-text+latex-positional/model\n",
      "0\tword2vec-text+latex-positional/cache/custom-en-constrained_positional_word2vec_cbow-epochs=10\n",
      "4,0K\tword2vec-text+latex-positional/cache\n",
      "472M\tword2vec-text+latex-positional\n",
      "472M\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -hc word2vec-text+latex-positional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84fe45-bd54-4df8-a723-421d33b99354",
   "metadata": {},
   "source": [
    "### The LaTeX format\n",
    "\n",
    "To train a `word2vec` model just for math, we also have a separate dataset with just LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c5a02c-4356-492d-8b89-d3241fb8d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make word2vec-latex-positional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e6bfa8b-25fe-4228-bed7-d79ae4dc5580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202M\tword2vec-latex-positional/model/custom-en-constrained_positional_word2vec_cbow-epochs=50\n",
      "202M\tword2vec-latex-positional/model\n",
      "0\tword2vec-latex-positional/cache/custom-en-constrained_positional_word2vec_cbow-epochs=50\n",
      "4,0K\tword2vec-latex-positional/cache\n",
      "202M\tword2vec-latex-positional\n",
      "202M\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -hc word2vec-latex-positional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f452dc-bfd5-4b20-9a2b-8ec61548133e",
   "metadata": {},
   "source": [
    "### The Tangent-L format\n",
    "\n",
    "To train a word2vec model just for math, we also have a separate dataset with just the format used by [the Tangent-L search engine from UWaterloo][1].\n",
    "\n",
    " [1]: http://ceur-ws.org/Vol-2936/paper-05.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "432b0dda-2f1b-48e8-8fb4-de9798052aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make word2vec-tangentl-positional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8fa3cc-27d9-4004-a85c-1990674e8dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14G\tword2vec-tangentl-positional/model/custom-en-constrained_positional_word2vec_cbow-epochs=2\n",
      "14G\tword2vec-tangentl-positional/model\n",
      "0\tword2vec-tangentl-positional/cache/custom-en-constrained_positional_word2vec_cbow-epochs=2\n",
      "4,0K\tword2vec-tangentl-positional/cache\n",
      "14G\tword2vec-tangentl-positional\n",
      "14G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -hc word2vec-tangentl-positional"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
