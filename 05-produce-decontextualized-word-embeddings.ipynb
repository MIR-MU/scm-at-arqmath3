{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd046cd-6f3d-47f1-93f1-fd7a48a3bd69",
   "metadata": {},
   "source": [
    "# Produce decontextualized word embeddings\n",
    "\n",
    "In this notebook, we will produce [decontextualized word embeddings][1] out of [the `roberta-base` model][2] and our fine-tuned version that can represent math-specific tokens.\n",
    "\n",
    " [1]: https://aclanthology.org/2021.wmt-1.112\n",
    " [2]: https://huggingface.co/roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9283262-7ae0-4753-b00e-2fe08890c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mir\n"
     ]
    }
   ],
   "source": [
    "! hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd142aa8-51ca-448f-b42e-3547c988a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install .[transformers,scm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88fdd1cb-5ad1-440a-b909-d34b37a41354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892d9fb-c447-430d-aae2-f05cce930f6d",
   "metadata": {},
   "source": [
    "## The `roberta-base` model\n",
    "\n",
    "First, we will extract decontextualized word embeddings out of the `roberta-base` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf38a81-ef5d-4c2c-ace9-c15e35349a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make decontextualized-word-embeddings-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d515f6fb-c172-4192-91a0-b643a7cafb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 novotny novotny 373K May 20 07:30 decontextualized-word-embeddings-roberta-base\n"
     ]
    }
   ],
   "source": [
    "%ls -lh decontextualized-word-embeddings-roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785573f-71f6-43c3-9ccb-b97eddccc842",
   "metadata": {},
   "source": [
    "To see how well the `roberta-base` model can represent scientific terms, we will load it and look at similar terms in the word embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eacf6317-e208-465f-804b-2c63e68a98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_embeddings = KeyedVectors.load('decontextualized-word-embeddings-roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8835a2b-0ba9-4990-8016-bf05da140c05",
   "metadata": {},
   "source": [
    "We can see that for scientific terms that have different meaning in math versus common usage, such as *absolute*, *property*, and *real*, `roberta-base` tends to favor the common usage:\n",
    "\n",
    "- *absolute*: total and complete *as opposed to* involving absolute values\n",
    "- *property*: someone's belongings/a building and the land belonging to it *as opposed to* an attribute of a math object\n",
    "- *real*: something that actually exists *as opposed to* involving or containing real numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641a3246-82a7-445b-9ff1-818a9fc12dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('absolutely', 0.9164975761832016),\n",
       " ('effective', 0.9140561937498846),\n",
       " ('extreme', 0.9050805288952175),\n",
       " ('actual', 0.9006715920201244),\n",
       " ('exclusive', 0.898657442002306),\n",
       " ('angular', 0.8981691067282201),\n",
       " ('relative', 0.8975861042260462),\n",
       " ('acceptable', 0.8964603402839815),\n",
       " ('offset', 0.8962548321691873),\n",
       " ('integer', 0.8945229985048906)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_embeddings.most_similar('absolute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70dd4956-2954-4741-b2d1-6ac1266d38c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Property', 0.9564183915543218),\n",
       " ('properties', 0.9490537038487242),\n",
       " ('perties', 0.9119670440793409),\n",
       " ('estate', 0.9104328288512181),\n",
       " ('value', 0.9087446331980998),\n",
       " ('pointer', 0.9086655706777714),\n",
       " ('theme', 0.9080653826613683),\n",
       " ('policy', 0.907620184092995),\n",
       " ('deck', 0.9064049816662365),\n",
       " ('attribute', 0.9063193512822962)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_embeddings.most_similar('property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9a3996-db41-43f9-af23-a5aad46a5574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Real', 0.9660123198743351),\n",
       " ('actual', 0.9356304996211505),\n",
       " ('reality', 0.9333833963212447),\n",
       " ('true', 0.9189921386453295),\n",
       " ('normal', 0.9181932473473009),\n",
       " ('natural', 0.9154994974594166),\n",
       " ('Normal', 0.9127584272831695),\n",
       " ('really', 0.9115041775006173),\n",
       " ('pal', 0.9112555406270034),\n",
       " ('legal', 0.9111350762652691)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_embeddings.most_similar('real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a46d7-eea7-4fab-8efb-cf84a6a209cd",
   "metadata": {},
   "source": [
    "Since our decontextualized token embeddings are the mean of contextual token embeddings on scientific texts, even `roberta-base` is not completely off-the-mark and includes similar scientific terms in the top ten most similar terms. However, `roberta-base` cannot always properly tokenize and attend the context in a scientific article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ec65b-c791-4dfa-90b0-8c565874f547",
   "metadata": {},
   "source": [
    "## The tuned `roberta-base` model\n",
    "\n",
    "Next, we will extract decontextualized word embeddings out of the `roberta-base` model fine-tuned so that it can represent math-specific tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb65de7b-c5a4-4cb8-b628-478e4d41b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make decontextualized-word-embeddings-tuned-roberta-base-text+latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18f5288e-b531-4921-b2fe-36c588163594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 novotny novotny 1.1M May 20 06:55 decontextualized-word-embeddings-tuned-roberta-base-text+latex\n"
     ]
    }
   ],
   "source": [
    "%ls -lh decontextualized-word-embeddings-tuned-roberta-base-text+latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2cae6-e215-4c65-89fc-7d0f3ebb708b",
   "metadata": {},
   "source": [
    "To see how well our fine-tuned model can represent scientific terms, we will load it and look at similar terms in the word embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc31fceb-6ace-4210-a047-67c176d47799",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_embeddings = KeyedVectors.load('decontextualized-word-embeddings-tuned-roberta-base-text+latex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea1387-acd9-4d78-9bb1-357dbcc4d33e",
   "metadata": {},
   "source": [
    "We can see that unlike `roberta-base`, our fine-tuned model tends to favor similar scientific terms:\n",
    "\n",
    "- *absolute*: top ten terms include fragments of mathematical operators rather than terms similar to the common usage\n",
    "- *property*: top ten terms do not include *value*, *estate*, or *policy*\n",
    "- *real*: top ten terms do not include *actual* and include *rational*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc50f932-3d20-4c4d-951b-9af29a363fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bra', 0.849663554118112),\n",
       " ('sqrt', 0.8285149035579248),\n",
       " ('gtr', 0.8284741846550531),\n",
       " ('framebox', 0.8179374852182264),\n",
       " ('frak', 0.8155009521481359),\n",
       " ('\\\\under', 0.814968915081996),\n",
       " ('alty', 0.8130912137629961),\n",
       " ('operatorname', 0.8126520618368384),\n",
       " ('operatorname*', 0.8112455505496647),\n",
       " ('inst', 0.8105142482596693)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_embeddings.most_similar('absolute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46fa7c2c-9a9d-405b-b67d-a81b372c662c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Property', 0.9384866797919728),\n",
       " ('properties', 0.9065765208383727),\n",
       " ('perty', 0.8977596035241844),\n",
       " ('attribute', 0.8971946003432749),\n",
       " ('element', 0.8941358996661748),\n",
       " ('entity', 0.8936722270445662),\n",
       " ('topic', 0.8895547168639073),\n",
       " ('perties', 0.8877034119576299),\n",
       " ('instance', 0.8855805134449843),\n",
       " ('functional', 0.8855760842701841)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_embeddings.most_similar('property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9cc3cee-e4ed-4381-aa72-ea352bfcd7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Real', 0.9148605082046193),\n",
       " ('reality', 0.9077418971319128),\n",
       " ('binary', 0.8970199766755064),\n",
       " ('rational', 0.8940688385836887),\n",
       " ('ral', 0.8923245888063248),\n",
       " ('urnal', 0.8919872449237995),\n",
       " ('mal', 0.8905495854135415),\n",
       " ('functional', 0.8905244359371303),\n",
       " ('mental', 0.8890649030130348),\n",
       " ('ual', 0.8888234435310957)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_embeddings.most_similar('real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de6b18-2f0b-4986-ad1b-c4b4f70683b8",
   "metadata": {},
   "source": [
    "Futhermore, unlike with `roberta-base`, we can use our fine-tuned model to find similar math-specific tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b492e8c-6ea6-46fb-8994-1e6d25d551da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('}\\\\cos', 0.956758189554533),\n",
       " ('\\\\sin', 0.9514626298264707),\n",
       " ('\\\\cos%', 0.9434441854751765),\n",
       " ('\\\\tan', 0.9278311303489227),\n",
       " ('}\\\\sin', 0.9254699807540067),\n",
       " ('(\\\\cos', 0.9244286133752913),\n",
       " ('\\\\cosh', 0.9221619260868039),\n",
       " ('\\\\frac{\\\\cos', 0.919889123507782),\n",
       " ('\\\\displaystyle\\\\cos', 0.9196743298469137),\n",
       " ('\\\\cos(', 0.9189877033669472)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_embeddings.most_similar(r'\\cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0509e964-9a18-4fcb-ba78-57bfc8a78733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('F(x)=', 0.9593355274441487),\n",
       " ('F(x', 0.9417396755668178),\n",
       " ('F(t)', 0.9369839133795281),\n",
       " ('G(x)', 0.9361178736043807),\n",
       " ('F(y', 0.9223169943227582),\n",
       " ('F(z', 0.9210098745370999),\n",
       " ('V(x)', 0.9149784694692186),\n",
       " ('F(X', 0.9138278425878289),\n",
       " ('F(', 0.913121134182822),\n",
       " ('F(u', 0.911264177551353)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_embeddings.most_similar('F(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5847621-209b-42fa-b365-de353574ac53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('}\\\\prod', 0.9532089875946642),\n",
       " ('\\\\displaystyle\\\\prod', 0.9342738865765974),\n",
       " ('}=\\\\prod', 0.9288030534822265),\n",
       " ('\\\\prod\\\\limits', 0.9229099580532697),\n",
       " ('\\\\sum', 0.9175814060521996),\n",
       " ('\\\\bigoplus', 0.9112117963203984),\n",
       " ('\\\\frac{\\\\prod', 0.9109938754752355),\n",
       " ('\\\\displaystyle\\\\sum', 0.9089826808485659),\n",
       " ('\\\\bigsqcup', 0.906801984020918),\n",
       " ('\\\\sum\\\\limits', 0.9052707240234698)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_embeddings.most_similar(r'\\prod')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
